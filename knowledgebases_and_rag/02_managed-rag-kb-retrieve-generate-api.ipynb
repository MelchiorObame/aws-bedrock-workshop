{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Q&A application using Amazon Bedrock Knowledge Bases - RetrieveAndGenerate API\n",
    "### Context\n",
    "\n",
    "With Amazon Bedrock Knowledge Bases, you can securely connect foundation models (FMs) in Amazon Bedrock to your company\n",
    "data for Retrieval Augmented Generation (RAG). Access to additional data helps the model generate more relevant,\n",
    "context-speciÔ¨Åc, and accurate responses without continuously retraining the FM. All information retrieved from\n",
    "Knowledge Bases comes with source attribution to improve transparency and minimize hallucinations. For more information on creating a Knowledge Base using the console, please refer to this [post](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base.html).\n",
    "\n",
    "In this notebook, we will dive deep into building a Q&A application using `RetrieveAndGenerate` API provided by Amazon Bedrock Knowledge Bases. This API will query the Knowledge Base to get the desired number of document chunks based on similarity search, integrate it with Large Language Model (LLM) for answering questions.\n",
    "\n",
    "\n",
    "### Pattern\n",
    "\n",
    "We can implement the solution using Retreival Augmented Generation (RAG) pattern. RAG retrieves data from outside the language model and augments the prompts by adding the relevant retrieved data in context. Here, we are performing RAG effectively on the Knowledge Base created in the previous notebook or using console. \n",
    "\n",
    "### Pre-requisite\n",
    "\n",
    "Before being able to answer the questions, the documents must be processed and stored in Knowledge Base.\n",
    "\n",
    "1. Load the documents into the Knowledge Base by connecting your s3 bucket (data source). \n",
    "2. Ingestion - Knowledge Base will split them into smaller chunks (based on the strategy selected), generate embeddings and store it in the associated vectore store and notebook [01_create_ingest_documents_test_kb.ipynb](./01_create_ingest_documents_test_kb.ipynb) takes care of it for you and creates two knowledgebases with the same source docs but different vector databases which we will analyze here.\n",
    "\n",
    "![data_ingestion.png](./images/data_ingestion.png)\n",
    "\n",
    "\n",
    "#### Notebook Walkthrough\n",
    "\n",
    "For our notebook we will use the `RetrieveAndGenerate API` provided by Amazon Bedrock Knowledge Bases which converts user queries into\n",
    "embeddings, searches the Knowledge Base, get the relevant results, augment the prompt and then invoking a LLM to generate the response. \n",
    "\n",
    "We will use the following workflow for this notebook. \n",
    "\n",
    "![retrieveAndGenerate.png](./images/retrieveAndGenerate.png)\n",
    "\n",
    "#### Use Case\n",
    "\n",
    "In this example, you will use several years of Amazon's Letter to Shareholders as a text corpus to perform Q&A on. This data is already ingested into the two Knowledge Bases. You will need the `Knowledge Base id` for both s3 vectors and AOSS, and `model ARN` to run this example. We are using `Amazon Nova Lite` model for generating responses to user questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both Knowledge Base IDs from previous notebook\n",
    "%store -r kb_id_aoss\n",
    "%store -r kb_id_s3vectors\n",
    "\n",
    "print(\"üìä Loaded Knowledge Base IDs:\")\n",
    "print(f\"  AOSS KB: {kb_id_aoss}\")\n",
    "print(f\"  S3 Vectors KB: {kb_id_s3vectors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pprint\n",
    "from botocore.client import Config\n",
    "import os\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "bedrock_config = Config(connect_timeout=120, read_timeout=120, retries={'max_attempts': 0})\n",
    "bedrock_client = boto3.client('bedrock-runtime', region_name='us-east-1')\n",
    "bedrock_agent_client = boto3.client(\"bedrock-agent-runtime\", region_name='us-east-1',config=bedrock_config)\n",
    "region_name = os.environ.get(\"AWS_DEFAULT_REGION\", \"us-east-1\")\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from util.model_selector import create_text_model_selector\n",
    "\n",
    "# Create interactive model selector\n",
    "model_selector = create_text_model_selector().display()\n",
    "# Get the selected model from our unified selector\n",
    "selected_model = model_selector.get_model_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve API\n",
    "Retrieve API converts user queries into embeddings, searches the Knowledge Base, and returns the relevant results, giving you more control to build custom workÔ¨Çows on top of the semantic search results. The output of the Retrieve API includes the the retrieved text chunks, the location type and URI of the source data, as well as the relevance scores of the retrievals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(input, kb_id, kb_name=\"Knowledge Base\"):\n",
    "    \"\"\"\n",
    "    Retrieve relevant documents from a Knowledge Base\n",
    "    \n",
    "    Args:\n",
    "        input: Query string\n",
    "        kb_id: Knowledge Base ID\n",
    "        kb_name: Name for logging purposes\n",
    "    \"\"\"\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # retrieve api for fetching only the relevant context.\n",
    "    relevant_documents = bedrock_agent_client.retrieve(\n",
    "        retrievalQuery={\n",
    "            'text': input\n",
    "        },\n",
    "        knowledgeBaseId=kb_id,\n",
    "        retrievalConfiguration={\n",
    "            'vectorSearchConfiguration': {\n",
    "                'numberOfResults': 3  # will fetch top 3 documents which matches closely with the query.\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"‚è±Ô∏è  {kb_name} retrieval time: {elapsed_time:.3f}s\")\n",
    "    \n",
    "    return relevant_documents[\"retrievalResults\"], elapsed_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is Amazon's doing in the field of generative AI?\"\n",
    "\n",
    "#AOSS backed KB\n",
    "response = retrieve(query, kb_id_aoss)\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#S3 vectors backed KB\n",
    "response = retrieve(query, kb_id_s3vectors)\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RetrieveAndGenerate API\n",
    "Behind the scenes, `RetrieveAndGenerate` API converts queries into embeddings, searches the Knowledge Base, and then augments the foundation model prompt with the search results as context information and returns the FM-generated response to the question. For multi-turn conversations, Knowledge Bases manage short-term memory of the conversation to provide more contextual results. \n",
    "\n",
    "The output of the `RetrieveAndGenerate` API includes the   `generated response`, `source attribution` as well as the `retrieved text chunks`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieveAndGenerate(input, kb_id, kb_name=\"Knowledge Base\", sessionId=None, model_id=selected_model):\n",
    "    \"\"\"\n",
    "    Retrieve and generate response from a Knowledge Base\n",
    "    \n",
    "    Args:\n",
    "        input: Query string\n",
    "        kb_id: Knowledge Base ID\n",
    "        kb_name: Name for logging purposes\n",
    "        sessionId: Optional session ID for conversation continuity\n",
    "        model_id: Model to use for generation\n",
    "    \"\"\"\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if sessionId:\n",
    "        response = bedrock_agent_client.retrieve_and_generate(\n",
    "            input={\n",
    "                'text': input\n",
    "            },\n",
    "            retrieveAndGenerateConfiguration={\n",
    "                'type': 'KNOWLEDGE_BASE',\n",
    "                'knowledgeBaseConfiguration': {\n",
    "                    'knowledgeBaseId': kb_id,\n",
    "                    'modelArn': model_id\n",
    "                }\n",
    "            },\n",
    "            sessionId=sessionId\n",
    "        )\n",
    "    else:\n",
    "        response = bedrock_agent_client.retrieve_and_generate(\n",
    "            input={\n",
    "                'text': input\n",
    "            },\n",
    "            retrieveAndGenerateConfiguration={\n",
    "                'type': 'KNOWLEDGE_BASE',\n",
    "                'knowledgeBaseConfiguration': {\n",
    "                    'knowledgeBaseId': kb_id,\n",
    "                    'modelArn': model_id\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"‚è±Ô∏è  {kb_name} total time: {elapsed_time:.3f}s\")\n",
    "    \n",
    "    return response, elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#AOSS backed KB\n",
    "response, elapsed_time = retrieveAndGenerate(query, kb_id_aoss, model_id=selected_model)\n",
    "generated_text = response['output']['text']\n",
    "pp.pprint(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citations = response[\"citations\"]\n",
    "contexts = []\n",
    "for citation in citations:\n",
    "    retrievedReferences = citation[\"retrievedReferences\"]\n",
    "    for reference in retrievedReferences:\n",
    "         contexts.append(reference[\"content\"][\"text\"])\n",
    "\n",
    "pp.pprint(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#S3 vectors backed KB\n",
    "response, elapsed_time = retrieveAndGenerate(query, kb_id_s3vectors, model_id=selected_model)\n",
    "generated_text = response['output']['text']\n",
    "pp.pprint(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citations = response[\"citations\"]\n",
    "contexts = []\n",
    "for citation in citations:\n",
    "    retrievedReferences = citation[\"retrievedReferences\"]\n",
    "    for reference in retrievedReferences:\n",
    "         contexts.append(reference[\"content\"][\"text\"])\n",
    "\n",
    "pp.pprint(contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Query Retrieve-Only Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries for comparison\n",
    "test_queries = [\n",
    "    \"What is Amazon's doing in the field of generative AI?\",\n",
    "    \"What is Graviton?\",\n",
    "    \"What are Amazon's key investments in AWS?\",\n",
    "    \"How did Amazon perform financially in 2022?\",\n",
    "    \"What is Amazon's approach to sustainability?\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üîç MULTI-QUERY RETRIEVE-ONLY COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Testing {len(test_queries)} queries with Retrieve API (no generation)\\n\")\n",
    "\n",
    "aoss_retrieve_times = []\n",
    "s3v_retrieve_times = []\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n[{i}/{len(test_queries)}] Query: {query}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Retrieve from AOSS\n",
    "    aoss_docs, aoss_time = retrieve(query, kb_id_aoss, \"AOSS\")\n",
    "    aoss_retrieve_times.append(aoss_time)\n",
    "    print(f\"  AOSS: Retrieved {len(aoss_docs)} chunks in {aoss_time:.3f}s\")\n",
    "    \n",
    "    # Retrieve from S3 Vectors\n",
    "    s3v_docs, s3v_time = retrieve(query, kb_id_s3vectors, \"S3V\")\n",
    "    s3v_retrieve_times.append(s3v_time)\n",
    "    print(f\"  S3V:  Retrieved {len(s3v_docs)} chunks in {s3v_time:.3f}s\")\n",
    "\n",
    "# Calculate statistics\n",
    "import statistics\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä RETRIEVE-ONLY PERFORMANCE STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nAOSS Knowledge Base (Retrieve Only):\")\n",
    "print(f\"  Average: {statistics.mean(aoss_retrieve_times):.3f}s\")\n",
    "print(f\"  Min:     {min(aoss_retrieve_times):.3f}s\")\n",
    "print(f\"  Max:     {max(aoss_retrieve_times):.3f}s\")\n",
    "print(f\"  StdDev:  {statistics.stdev(aoss_retrieve_times):.3f}s\")\n",
    "\n",
    "print(f\"\\nS3 Vectors Knowledge Base (Retrieve Only):\")\n",
    "print(f\"  Average: {statistics.mean(s3v_retrieve_times):.3f}s\")\n",
    "print(f\"  Min:     {min(s3v_retrieve_times):.3f}s\")\n",
    "print(f\"  Max:     {max(s3v_retrieve_times):.3f}s\")\n",
    "print(f\"  StdDev:  {statistics.stdev(s3v_retrieve_times):.3f}s\")\n",
    "\n",
    "print(f\"\\nüí° Retrieve-Only Insights:\")\n",
    "avg_diff = abs(statistics.mean(aoss_retrieve_times) - statistics.mean(s3v_retrieve_times))\n",
    "print(f\"  Average difference: {avg_diff:.3f}s\")\n",
    "if statistics.mean(aoss_retrieve_times) < statistics.mean(s3v_retrieve_times):\n",
    "    pct = ((statistics.mean(s3v_retrieve_times)/statistics.mean(aoss_retrieve_times) - 1) * 100)\n",
    "    print(f\"  ‚úÖ AOSS is {pct:.1f}% faster for retrieval\")\n",
    "else:\n",
    "    pct = ((statistics.mean(aoss_retrieve_times)/statistics.mean(s3v_retrieve_times) - 1) * 100)\n",
    "    print(f\"  ‚úÖ S3 Vectors is {pct:.1f}% faster for retrieval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Query RetrieveAndGenerate Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n\" + \"=\" * 80)\n",
    "print(\"ü§ñ MULTI-QUERY RETRIEVE-AND-GENERATE COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Testing {len(test_queries)} queries with RetrieveAndGenerate API\\n\")\n",
    "\n",
    "aoss_rag_times = []\n",
    "s3v_rag_times = []\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n[{i}/{len(test_queries)}] Query: {query}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # RetrieveAndGenerate from AOSS\n",
    "    aoss_response, aoss_time = retrieveAndGenerate(query, kb_id_aoss, \"AOSS\", model_id=selected_model)\n",
    "    aoss_rag_times.append(aoss_time)\n",
    "    aoss_answer = aoss_response['output']['text']\n",
    "    print(f\"  AOSS: Generated answer in {aoss_time:.3f}s\")\n",
    "    print(f\"        Answer preview: {aoss_answer[:100]}...\")\n",
    "    \n",
    "    # RetrieveAndGenerate from S3 Vectors\n",
    "    s3v_response, s3v_time = retrieveAndGenerate(query, kb_id_s3vectors, \"S3V\", model_id=selected_model)\n",
    "    s3v_rag_times.append(s3v_time)\n",
    "    s3v_answer = s3v_response['output']['text']\n",
    "    print(f\"  S3V:  Generated answer in {s3v_time:.3f}s\")\n",
    "    print(f\"        Answer preview: {s3v_answer[:100]}...\")\n",
    "\n",
    "# Calculate statistics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä RETRIEVE-AND-GENERATE PERFORMANCE STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nAOSS Knowledge Base (Retrieve + Generate):\")\n",
    "print(f\"  Average: {statistics.mean(aoss_rag_times):.3f}s\")\n",
    "print(f\"  Min:     {min(aoss_rag_times):.3f}s\")\n",
    "print(f\"  Max:     {max(aoss_rag_times):.3f}s\")\n",
    "print(f\"  StdDev:  {statistics.stdev(aoss_rag_times):.3f}s\")\n",
    "\n",
    "print(f\"\\nS3 Vectors Knowledge Base (Retrieve + Generate):\")\n",
    "print(f\"  Average: {statistics.mean(s3v_rag_times):.3f}s\")\n",
    "print(f\"  Min:     {min(s3v_rag_times):.3f}s\")\n",
    "print(f\"  Max:     {max(s3v_rag_times):.3f}s\")\n",
    "print(f\"  StdDev:  {statistics.stdev(s3v_rag_times):.3f}s\")\n",
    "\n",
    "print(f\"\\nüí° Retrieve-And-Generate Insights:\")\n",
    "avg_diff = abs(statistics.mean(aoss_rag_times) - statistics.mean(s3v_rag_times))\n",
    "print(f\"  Average difference: {avg_diff:.3f}s\")\n",
    "if statistics.mean(aoss_rag_times) < statistics.mean(s3v_rag_times):\n",
    "    pct = ((statistics.mean(s3v_rag_times)/statistics.mean(aoss_rag_times) - 1) * 100)\n",
    "    print(f\"  ‚úÖ AOSS is {pct:.1f}% faster end-to-end\")\n",
    "    print(f\"  üìå Best for: Ultra-low latency requirements\")\n",
    "else:\n",
    "    pct = ((statistics.mean(aoss_rag_times)/statistics.mean(s3v_rag_times) - 1) * 100)\n",
    "    print(f\"  ‚úÖ S3 Vectors is {pct:.1f}% faster end-to-end\")\n",
    "    print(f\"  üìå Best for: Cost-effective large-scale deployments\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n\" + \"=\" * 80)\n",
    "print(\"üìà OVERALL PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n{'Metric':<30} {'AOSS':<15} {'S3 Vectors':<15} {'Difference':<15}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Retrieve Only (avg)':<30} {statistics.mean(aoss_retrieve_times):.3f}s{'':<9} {statistics.mean(s3v_retrieve_times):.3f}s{'':<9} {abs(statistics.mean(aoss_retrieve_times) - statistics.mean(s3v_retrieve_times)):.3f}s\")\n",
    "print(f\"{'Retrieve + Generate (avg)':<30} {statistics.mean(aoss_rag_times):.3f}s{'':<9} {statistics.mean(s3v_rag_times):.3f}s{'':<9} {abs(statistics.mean(aoss_rag_times) - statistics.mean(s3v_rag_times)):.3f}s\")\n",
    "\n",
    "print(f\"\\nüéØ Key Takeaways:\")\n",
    "print(f\"  ‚Ä¢ Both Knowledge Bases return similar quality results (same embeddings & chunks)\")\n",
    "print(f\"  ‚Ä¢ AOSS optimized for millisecond latency, higher cost\")\n",
    "print(f\"  ‚Ä¢ S3 Vectors optimized for cost efficiency, sub-second latency\")\n",
    "print(f\"  ‚Ä¢ Choose based on your latency requirements and budget\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Comparison: AOSS vs S3 Vectors\n",
    "\n",
    "### Amazon OpenSearch Serverless (AOSS)\n",
    "- **Pricing Model**: OCU (OpenSearch Compute Units) based\n",
    "- **Indexing**: ~$0.24/OCU-hour\n",
    "- **Search**: ~$0.24/OCU-hour\n",
    "- **Storage**: ~$0.024/GB-month\n",
    "- **Best For**: Applications requiring millisecond latency\n",
    "\n",
    "### Amazon S3 Vectors (Preview)\n",
    "- **Pricing Model**: Storage + query based\n",
    "- **Storage**: S3 Standard pricing (~$0.023/GB-month)\n",
    "- **Queries**: Pay per query\n",
    "- **Best For**: Large-scale, cost-sensitive applications with sub-second latency requirements\n",
    "\n",
    "### When to Choose Each:\n",
    "\n",
    "**Choose AOSS when:**\n",
    "- You need millisecond query latency\n",
    "- You have complex filtering requirements\n",
    "- You need real-time updates\n",
    "- Budget allows for higher compute costs\n",
    "\n",
    "**Choose S3 Vectors when:**\n",
    "- You have large vector datasets (millions+)\n",
    "- Sub-second latency is acceptable\n",
    "- Cost optimization is a priority\n",
    "- You want seamless S3 integration\n",
    "\n",
    "### Performance vs Cost Trade-off:\n",
    "- AOSS: Higher cost, lower latency (milliseconds)\n",
    "- S3 Vectors: Lower cost, slightly higher latency (sub-second)\n",
    "\n",
    "Both solutions provide excellent retrieval quality with the same embedding model and chunking strategy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Challenge\n",
    "- Based on the Knowledge Base you created in the previous additional challenge (01_create_ingest_documents_test_\n",
    "kb.ipynb), add metadata filtering capabilities to the retrieval operation using the python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" METADATA FILTERING RETRIVAL\"\"\"\n",
    "\n",
    "def retrieve_with_metadata_filtering(input, kb_id, kb_name=\"Knowledge Base\", metadata_filter=None):\n",
    "    \"\"\"\n",
    "    Retrieve relevant documents from a Knowledge Base with metadata filtering implementation\n",
    "    \n",
    "    Args:\n",
    "        input: Query string\n",
    "        kb_id: Knowledge Base ID\n",
    "        kb_name: Name for logging purposes\n",
    "        metadata_filter: Optional metadata filter dict\n",
    "    \"\"\"\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Build retrieval configuration\n",
    "    retrieval_config = {\n",
    "        'vectorSearchConfiguration': {\n",
    "            'numberOfResults': 3\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add metadata filter if provided\n",
    "    if metadata_filter:\n",
    "        retrieval_config['vectorSearchConfiguration']['filter'] = metadata_filter\n",
    "    \n",
    "    # retrieve api for fetching only the relevant context.\n",
    "    relevant_documents = bedrock_agent_client.retrieve(\n",
    "        retrievalQuery={\n",
    "            'text': input\n",
    "        },\n",
    "        knowledgeBaseId=kb_id,\n",
    "        retrievalConfiguration=retrieval_config\n",
    "    )\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"‚è±Ô∏è  {kb_name} retrieval time: {elapsed_time:.3f}s\")\n",
    "    \n",
    "    return relevant_documents[\"retrievalResults\"], elapsed_time\n",
    "\n",
    "# Meta data filters examples\n",
    "# metadata_filter = {\"equals\": {\"key\": \"document_type\", \"value\": \"financial_report\"}}\n",
    "# metadata_filter = {\"greaterThan\": {\"key\": \"year\", \"value\": 2020}}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
